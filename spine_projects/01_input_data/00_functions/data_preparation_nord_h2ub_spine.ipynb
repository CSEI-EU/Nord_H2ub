{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43ea4b8f-0ba4-4513-8872-53f4a538a0d3",
   "metadata": {},
   "source": [
    "# Data Preparation for the Nord_H2ub Spine Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa86773-4431-4a2f-9490-4b3cf0aa8d70",
   "metadata": {},
   "source": [
    "This jupyter notebook contains all routines for the preparation of the input data sources into a input data file for the model in Spine. \n",
    "\n",
    "**Authors:** Johannes Giehl (jfg.eco@cbs.dk), Dana Hentschel (djh.eco@cbs.dk), Lucia Ciprian (luc.eco@cbs.dk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8175ae-f246-4788-a309-c1a4890992f2",
   "metadata": {},
   "source": [
    "## General settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc68f78-3383-4889-bd9e-8b3e3e2f6c2e",
   "metadata": {},
   "source": [
    "### Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f47a1c-073f-46c0-8a4d-7dafb1716b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92141795-3716-404e-8e35-a096dcaa0c40",
   "metadata": {},
   "source": [
    "### Methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23418a81-7e7c-47c4-afc1-805a5eb7aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data preparation functions\n",
    "# Determine the current working directory\n",
    "module_path = os.getcwd()\n",
    "\n",
    "if os.path.basename(module_path) != '00_functions':\n",
    "# Set the module path (adjust the relative path if necessary)\n",
    "    module_path = os.path.abspath(os.path.join(module_path, '00_functions'))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "# Load the functions and methods from the corresponding file\n",
    "from nord_h2ub_data_preparation_functions import *\n",
    "from nord_h2ub_data_preparation_main_functions import *\n",
    "from nord_h2ub_data_preparation_helper_functions import *\n",
    "from nord_h2ub_data_preparation_investment_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e50630f-868b-40f0-842e-c74d936b5d54",
   "metadata": {},
   "source": [
    "### Base parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f557c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this should just be used when running it from the user interface\n",
    "with open(os.path.join(module_path, 'parameters.pkl'), 'rb') as file:\n",
    "    parameters = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b565675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not parameters:  \n",
    "    print(\"Warning: No parameters were injected. Using default parameters only.\")\n",
    "\n",
    "(year, starting_year, lcoe_years, start_date, end_date, area, product, demand, demand_res,\n",
    " powers, powers_capacities, frequency, \n",
    " model_name, temporal_block, \n",
    " stochastic_scenario, stochastic_structure, run_name, report_name, reports, \n",
    " electrolyzer_type, des_segments_electrolyzer, \n",
    " wacc, share_of_dh_price_cap, price_level_power, power_price_variance, \n",
    " roll_forward_use, roll_forward_size, num_slices, datetime_index, \n",
    " candidate_nonzero, \n",
    " investment_period_default, investment_cost_params, investment_limit_params,\n",
    " capacities_existing_params, investment_res, investment_ps) = set_parameters(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f03f1d2b-f4ad-4bd0-b44e-4d86263a37b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inv_cost_Ammonia_storage': None,\n",
       " 'inv_cost_Anaerobic': None,\n",
       " 'inv_cost_Air_separation_unit': None,\n",
       " 'inv_cost_Biomethanation': None,\n",
       " 'inv_cost_CO2_Vaporizer': None,\n",
       " 'inv_cost_Diesel_storage': None,\n",
       " 'inv_cost_Egasoline_storage': None,\n",
       " 'inv_cost_Electric_Steam_Boiler': None,\n",
       " 'inv_cost_Electrolyzer': '1,400,000',\n",
       " 'inv_cost_Fischer_Tropsch_unit': None,\n",
       " 'inv_cost_Haber_Bosch_reactor': None,\n",
       " 'inv_cost_Hydrogen_storage': None,\n",
       " 'inv_cost_Jet_fuel_storage': None,\n",
       " 'inv_cost_Methane_storage': None,\n",
       " 'inv_cost_Methanol_Plant': None,\n",
       " 'inv_cost_Methanol_storage': None,\n",
       " 'inv_cost_RWGS_unit': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "investment_cost_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c56b541-1fbd-4e7e-9cf3-2ff25f8da472",
   "metadata": {},
   "source": [
    "### File paths\n",
    "\n",
    "Set path to correct folders and set the names of the relevant files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b811bdc-ac7b-4cd7-8e32-8f84bda412a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "excel_file_path = get_excel_file_path() + '/01_input_raw/'\n",
    "# Prepared input data\n",
    "output_file_path = get_excel_file_path() + '/02_input_prepared/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89169e64-f204-47bb-9a83-17156a4aefa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "Model_structure_file = '/Model_Data_Base_' + product + '.xlsx'\n",
    "efficiency_electrolyzer_file = 'Efficiency_Electrolyzers.xlsx'\n",
    "distric_heating_price_file = '/Energy Prices/district_heating_price_cap.xlsx'\n",
    "investment_costs_file = '/investment_cost_overview/Investment_cost_overview.xlsx'\n",
    "\n",
    "PV_data_availabilityfactors = 'Availabilities Renewables/PV_availability_factors_' + area + '.csv'\n",
    "Wind_data_availabilityfactors = 'Availabilities Renewables/Wind_availability_factors_' + area + '.csv'\n",
    "data_powerprices = 'Energy Prices/Day_ahead_prices.xlsx'\n",
    "grid_costs = '/Energy Prices/grid_costs.xlsx'\n",
    "other_costs = '/Energy Prices/other_inout_costs.xlsx'\n",
    "\n",
    "# Output data\n",
    "output_file_name = product + '_Input_prepared.xlsx'\n",
    "output_mapping_file_name = product + '_object_mapping.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d15fe8-064f-4c28-be28-8d924a4ba31b",
   "metadata": {},
   "source": [
    "\n",
    "## Workflow of the data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ad49f-470e-4b29-aa02-c7babb08befb",
   "metadata": {},
   "source": [
    "### General parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f50c3-e43d-41c2-8158-75596632213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date index\n",
    "date_index = pd.date_range(start=start_date, end=end_date, freq='h')\n",
    "formatted_dates = date_index.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "df_formatted_dates = pd.DataFrame(formatted_dates, columns=['DateTime'])\n",
    "\n",
    "df_time = pd.DataFrame(df_formatted_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fb746a-7cd3-4655-88cf-ac543b003918",
   "metadata": {},
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42c5077-74ef-4f63-b6d9-4aa3c11aa878",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "\n",
    "# Model structure\n",
    "df_model_units = pd.read_excel(excel_file_path + '/Products/' + product + Model_structure_file, sheet_name='Units', index_col=None)\n",
    "df_model_units_relations = pd.read_excel(excel_file_path + '/Products/' + product + Model_structure_file, sheet_name='Unit_relations', index_col=None)\n",
    "df_model_connections = pd.read_excel(excel_file_path + '/Products/' + product + Model_structure_file, sheet_name='Connections', index_col=None)\n",
    "df_model_storages = pd.read_excel(excel_file_path + '/Products/' + product + Model_structure_file, sheet_name='Storages', index_col=None)\n",
    "\n",
    "# Variable efficiency\n",
    "df_efficiency_electrolyzer = pd.read_excel(excel_file_path + efficiency_electrolyzer_file, sheet_name='Efficiency_'+electrolyzer_type)\n",
    "\n",
    "# Availability factor\n",
    "df_PV_availabilityfactors_values = pd.read_csv(excel_file_path+PV_data_availabilityfactors, skiprows=3, usecols=[0,1,2])\n",
    "df_wind_availabilityfactors_values = pd.read_csv(excel_file_path+Wind_data_availabilityfactors, skiprows=3, usecols=[0,1,2,3])\n",
    "\n",
    "# Power prices\n",
    "df_powerprices_total_values = pd.read_excel(excel_file_path+data_powerprices, sheet_name=str(year))\n",
    "df_powerprices_values = df_powerprices_total_values[['HourDK', str(area)]].copy()\n",
    "df_grid_costs = pd.read_excel(excel_file_path + grid_costs)\n",
    "\n",
    "# District heating prices\n",
    "df_other_costs = pd.read_excel(excel_file_path + other_costs, index_col = None)\n",
    "df_district_heating_price = pd.read_excel(excel_file_path + distric_heating_price_file, sheet_name='Price_Cap_Calculation', index_col=None)\n",
    "\n",
    "# Investment costs\n",
    "df_investment_costs_raw = pd.read_excel(excel_file_path + investment_costs_file, sheet_name='Investment_Cost', index_col=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2693fc2-fe36-47a1-9b08-00f73dd53b9f",
   "metadata": {},
   "source": [
    "### Adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73718c6",
   "metadata": {},
   "source": [
    "#### Add powers information to database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861e7dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to dictionary\n",
    "capacities_existing_params['capacity_PV_plant'] = powers_capacities['Solar plant']\n",
    "capacities_existing_params['capacity_Wind_onshore'] = powers_capacities['Wind onshore']\n",
    "capacities_existing_params['capacity_Wind_offshore'] = powers_capacities['Wind offshore']\n",
    "\n",
    "# Remove missing power source if necessary\n",
    "# Step 1: Check for Solar plant\n",
    "if 'Solar plant' not in powers:\n",
    "    df_model_units = df_model_units.loc[df_model_units['Object_type'] != 'PV_plant']\n",
    "    df_model_units_relations = df_model_units_relations.loc[df_model_units_relations['Object_type'] != 'PV_plant']\n",
    "\n",
    "# Step 2: Check for Wind_onshore\n",
    "if 'Wind onshore' not in powers:\n",
    "    df_model_units = df_model_units.loc[df_model_units['Object_type'] != 'Wind_onshore']\n",
    "    df_model_units_relations = df_model_units_relations.loc[df_model_units_relations['Object_type'] != 'Wind_onshore']\n",
    "\n",
    "# Step 3: Check for Wind_offshore\n",
    "if 'Wind offshore' not in powers:\n",
    "    df_model_units = df_model_units.loc[df_model_units['Object_type'] != 'Wind_offshore']\n",
    "    df_model_units_relations = df_model_units_relations.loc[df_model_units_relations['Object_type'] != 'Wind_offshore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d10b543-5e6f-4691-8826-9dbb20ecb5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Energy Storage if selected\n",
    "if investment_ps:\n",
    "    if not (df_model_storages['Object_type'] == 'Power_storage').any():\n",
    "        # Add to storages\n",
    "        new_row = {col: np.nan for col in df_model_storages.columns}  # Initialize all columns with NaN\n",
    "        new_row['Storage'] = 'power_st'\n",
    "        new_row['Object_type'] = 'Power_storage'\n",
    "        new_row['value_before'] = 0\n",
    "        new_row['has_state'] = 'True'\n",
    "        new_row['node_state_cap'] = investment_limit_params['investment_ps_capacity']\n",
    "        new_row['node_state'] = 'fix_node_state'\n",
    "        new_row['initial_storages_invested'] = 0\n",
    "        new_row_df = pd.DataFrame([new_row])\n",
    "        df_model_storages = pd.concat([df_model_storages, new_row_df], ignore_index = True)\n",
    "        \n",
    "        # Add connections\n",
    "        new_row_c = df_model_connections[(df_model_connections['Object_type'] == 'Power_line') & (df_model_connections['Input2'] == 'power')].copy()\n",
    "        new_row_c['Connection'] = 'pl_power_st'\n",
    "        new_row_c['Input1'] = 'power_st'\n",
    "        new_row_c['Output2'] = 'power_st'\n",
    "        new_row_c['Connection_type'] = 'connection_type_normal'\n",
    "        df_model_connections = pd.concat([df_model_connections, new_row_c], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc063ea-e5f2-4592-b523-6be1c8375f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Grid power is unchecked and if so, remove from db\n",
    "if 'Grid' not in powers:\n",
    "    df_model_connections = df_model_connections[~df_model_connections['Connection'].str.contains(\"wholesale\", case=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad357543-0976-4e90-b3d6-fe1aba7f730d",
   "metadata": {},
   "source": [
    "### Add updated demand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef703db-8cf5-404b-ba48-de315ca673af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update demand column and temporal resolution\n",
    "product_tr = translate_product(product)\n",
    "demand_res_tr = next((key for key, value in resolution_to_block.items() if value == demand_res), None)\n",
    "\n",
    "# Get last unit:\n",
    "last_unit = (df_model_units_relations.loc[df_model_units_relations['Output1'] == product_tr, 'Unit']).iloc[0]\n",
    "\n",
    "# Look for row that contains the last unit and then replace demand value with new value\n",
    "df_model_units.loc[df_model_units['Unit'] == last_unit, 'demand'] = demand / ((365 + calendar.isleap(year)) * 24)\n",
    "\n",
    "df_model_units['resolution_output'] = df_model_units['resolution_output'].astype(str)\n",
    "df_model_units.loc[df_model_units['Unit'] == last_unit, 'resolution_output'] = demand_res_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28376bad-520d-4ab3-85ce-2d1cf91ac5e5",
   "metadata": {},
   "source": [
    "#### Adjust base elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfe4eb9-681a-4574-8170-cdc08156460b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping tables for object name to type\n",
    "df_model_units_mapping = df_model_units[['Unit', 'Object_type']].copy()\n",
    "df_model_connections_mapping = df_model_connections[['Connection', 'Object_type']].copy()\n",
    "df_model_storages_mapping = df_model_storages[['Storage', 'Object_type']].copy()\n",
    "\n",
    "df_model_units_mapping.rename(columns={'Unit': 'Object_name'}, inplace=True)\n",
    "df_model_connections_mapping.rename(columns={'Connection': 'Object_name'}, inplace=True)\n",
    "df_model_storages_mapping.rename(columns={'Storage': 'Object_name'}, inplace=True)\n",
    "\n",
    "# Create a dataframe with mapping of all object in the model\n",
    "df_model_object_mapping = pd.concat([df_model_units_mapping, df_model_connections_mapping, df_model_storages_mapping], axis=0)\n",
    "df_model_object_mapping = df_model_object_mapping.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd925e8-09bd-4d8a-9563-39423bf0c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the investment information from the user interface into a dataframe\n",
    "df_filtered_investment_cost = filter_investment_data(name_parameter='investment_cost', **investment_cost_params)\n",
    "df_filtered_investment_limit = filter_investment_data(name_parameter='investment_limit', **investment_limit_params)\n",
    "df_filtered_existing_cap = filter_investment_data(name_parameter='capacities_existing', **capacities_existing_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef48f04c-3560-4abb-a54e-9143a869847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove prefix\n",
    "df_investment_limit = remove_prefix(df_filtered_investment_limit, 'inv_limit_')\n",
    "df_investment_cost = remove_prefix(df_filtered_investment_cost, 'inv_cost_')\n",
    "df_existing_cap = remove_prefix(df_filtered_existing_cap, 'capacity_')\n",
    "\n",
    "# Map to object name\n",
    "df_investment_limit = map_type_to_name(df_investment_limit, df_model_object_mapping)\n",
    "df_investment_cost = map_type_to_name(df_investment_cost, df_model_object_mapping)\n",
    "df_existing_cap = map_type_to_name(df_existing_cap, df_model_object_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c95ef6-87b3-4609-84c4-ceb9d56bec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_investment_params = pd.merge(\n",
    "    df_investment_limit[['Object_name', 'Object_type', 'investment_limit']], df_investment_cost[['Object_name', 'investment_cost']],\n",
    "    on='Object_name', how='outer'\n",
    ")\n",
    "df_investment_params = pd.merge(\n",
    "    df_investment_params, df_existing_cap[['Object_name', 'Object_type', 'capacities_existing']],\n",
    "    on='Object_name', how='outer'\n",
    ")\n",
    "\n",
    "# Combine object_type columns\n",
    "df_investment_params['Object_type'] = df_investment_params['Object_type_x'].combine_first(df_investment_params['Object_type_y'])\n",
    "# Drop old, redundant columns\n",
    "df_investment_params.drop(['Object_type_x', 'Object_type_y'], axis=1, inplace=True)\n",
    "\n",
    "#update the values of RES in the investment params based on the user input\n",
    "#relevant to set the limit and existing capacity in this df\n",
    "df_investment_params = update_res_parameter_in_invest(df_investment_params, powers_capacities, 'capacities_existing')\n",
    "df_investment_params = update_res_parameter_in_invest(df_investment_params, powers_capacities, 'investment_limit')\n",
    "\n",
    "# Print for control\n",
    "df_investment_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e86607-2ad3-4aa8-8bbd-3bee30a62970",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adjust the storage loss rate values to fit to the SpineOpt implementation\n",
    "df_model_storages = adjust_frac_state_loss(df_model_storages, 'frac_state_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7ec120-9bbf-40af-ac29-9b429051d1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract connections into a separate df\n",
    "df_connections = process_dataframe(df_model_connections, 'Connection', 'connection')\n",
    "\n",
    "# Define the elements of the network\n",
    "df_definition, df_nodes = create_definition_dataframe(df_model_units_relations, df_model_connections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287b3c37-228f-47f0-a99e-2df1f87512d5",
   "metadata": {},
   "source": [
    "## Fitting data into format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9140ca0",
   "metadata": {},
   "source": [
    "### Object parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c90d86-d62f-49aa-bf8c-a7ceec921d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame for all parameters of units\n",
    "\n",
    "# Add unit fom costs\n",
    "unit_fom_cost_df = create_fom_units(df_model_units, df_investment_costs_raw, year, investment_res, run_name)\n",
    "# Add unit minimal downtime\n",
    "unit_min_down_time_df = create_object_parameters(df_model_units, 'Unit', 'min_down_time', run_name)\n",
    "# Add unit on cost\n",
    "unit_on_cost_df = create_object_parameters(df_model_units, 'Unit', 'units_on_cost', run_name)\n",
    "# Add start up costs\n",
    "start_up_cost_df = create_object_parameters(df_model_units, 'Unit', 'start_up_cost', run_name)\n",
    "# Add shut down costs\n",
    "shut_down_cost_df = create_object_parameters(df_model_units, 'Unit', 'shut_down_cost', run_name)\n",
    "# Add initial_units_on\n",
    "initial_units_on_df = create_object_parameters(df_model_units, 'Unit', 'initial_units_on', run_name)\n",
    "\n",
    "connection_fom_cost_df = create_object_parameters(df_model_connections, 'Connection', 'fom_cost', run_name)\n",
    "\n",
    "# Create a complete data frame with all parameters\n",
    "unit_parameters_df = pd.concat([unit_fom_cost_df, unit_min_down_time_df, unit_on_cost_df, start_up_cost_df, shut_down_cost_df, initial_units_on_df, connection_fom_cost_df], ignore_index=True)\n",
    "\n",
    "# Show table head for control\n",
    "unit_parameters_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470215f7-7b7f-45f3-a1b6-daa11589ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new data frame for parameters that are given as durations\n",
    "# Necessary as SpineToolbox needs a separate input to map the parameter correctly\n",
    "\n",
    "duration_parameter = 'min_down_time'\n",
    "unit_parameters_duration_df = unit_parameters_df[unit_parameters_df['Parameter'] == duration_parameter]\n",
    "# Resetting the index\n",
    "unit_parameters_duration_df = unit_parameters_duration_df.reset_index(drop=True)\n",
    "\n",
    "# Creating another DataFrame with rows that do not meet the condition\n",
    "unit_parameters_rest_df = unit_parameters_df[unit_parameters_df['Parameter'] != duration_parameter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4e42d4-76d0-480d-b7b2-bdb413adfc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the balance type of the nodes\n",
    "columns_to_select1 = ['Input1', 'Input2', 'Input3', 'Input4','Output1', 'Output2', 'Output3', 'Output4']\n",
    "columns_to_select2 = ['Input1', 'Input2','Output1', 'Output2']\n",
    "#df_combined = pd.concat([df_model_units_relations, df_model_connections])\n",
    "#df_combined = df_combined.reset_index(drop=True)\n",
    "\n",
    "df_nodes_network = create_connection_dataframe(df_model_units_relations, df_model_connections, columns_to_select1, columns_to_select2)\n",
    "\n",
    "# Get unique values from the 'in' column\n",
    "unique_in_values = df_nodes_network['in'].dropna().unique()\n",
    "\n",
    "# Identify values in 'in' column not present in 'out' column\n",
    "values_not_in_out = unique_in_values[~pd.Series(unique_in_values).isin(df_nodes_network['out'].dropna().unique())]\n",
    "\n",
    "# Get unique values from the 'in' column\n",
    "unique_out_values = df_nodes_network['out'].dropna().unique()\n",
    "\n",
    "# Identify values in 'in' column not present in 'out' column\n",
    "values_not_in_in = unique_out_values[~pd.Series(unique_out_values).isin(df_nodes_network['in'].dropna().unique())]\n",
    "\n",
    "# Create list of unique nodes that are either start or end nodes\n",
    "unique_nodes = values_not_in_out.tolist() + values_not_in_in.tolist()\n",
    "unique_nodes\n",
    "\n",
    "df_nodes_network.replace(np.nan, None, inplace=True)\n",
    "\n",
    "# Check for combinations that are mirrored\n",
    "mirrored_combinations = find_mirror_combinations(df_nodes_network)\n",
    "\n",
    "# Get information of connections for each node\n",
    "partners_dict1 = find_partners(df_nodes_network)\n",
    "partners_dict2 = find_partners(mirrored_combinations)\n",
    "\n",
    "# Check both lists if there are identical entries and list nodes that only have a connection to the same node\n",
    "# Storages are removed as they must be balanced\n",
    "nodes_identical = find_identical_entries(partners_dict1, partners_dict2)\n",
    "\n",
    "# Combined list of start and end nodes that should be unbalanced\n",
    "unbalanced_nodes = nodes_identical + unique_nodes\n",
    "\n",
    "df_nodes['balance_type'] = 'balance_type_node'\n",
    "df_nodes.loc[df_nodes['Object_name'].isin(unbalanced_nodes), 'balance_type'] = 'balance_type_none'\n",
    "\n",
    "df_nodes['Alternative'] = run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5971a5-89b2-4d80-bc6b-b5a07b5b2de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the node of internal heat to have a nodal_balance_sense of \">=\" -> to be taken out once we use the heat.\n",
    "df_nodes['nodal_balance_sense'] = \"\"\n",
    "df_nodes.loc[df_nodes['Object_name'] == 'heat_high', 'nodal_balance_sense'] = '>='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0403b80a-9d3f-4557-bda6-2803650bea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add has_state_node_state_cap and frac_state_loss\n",
    "df_storages_short = df_model_storages.loc[:, ['Storage', 'has_state', 'node_state_cap', 'frac_state_loss']].rename(columns={'Storage': 'Object_name'})\n",
    "df_storages_short['has_state'] = df_storages_short['has_state'].astype(str).str.lower().replace('true', 'true')\n",
    "\n",
    "df_nodes = pd.merge(df_nodes, df_storages_short, on='Object_name', how='left')\n",
    "df_nodes['demand'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb44a55f-bd0d-41f3-a96a-2fc67ae92a5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a dataframe with all nodes that should have a slack based on the unit input and outputs\n",
    "nodes_for_slack_df = check_entries_exist(df_model_units_relations, 'node')\n",
    "\n",
    "# Merge the information into the prepared data frame\n",
    "merged_df = pd.merge(df_nodes, nodes_for_slack_df, left_on='Object_name', right_on='node', how='left')\n",
    "merged_df = merged_df.drop(columns=['node'])\n",
    "merged_df['node_slack_penalty'] = merged_df['node_slack_penalty'].replace({True: 100000000, False: ''})\n",
    "# Create a dataframe with all nodes that should have a slack based on the connection input and outputs\n",
    "nodes_for_slack_df2 = check_entries_exist(df_model_connections, 'connection')\n",
    "# Merge the information into the prepared data frame\n",
    "merged_df2 = pd.merge(df_nodes, nodes_for_slack_df2, left_on='Object_name', right_on='connection', how='left')\n",
    "merged_df2 = merged_df2.drop(columns=['connection'])\n",
    "merged_df2['node_slack_penalty'] = merged_df2['node_slack_penalty'].replace({True: 100000000, False: ''})\n",
    "# Link the information about the slack of both data frames\n",
    "merged_df2['node_slack_penalty'] = merged_df['node_slack_penalty'].combine_first(merged_df2['node_slack_penalty'])\n",
    "# Clean the information that only nodes with 'balance_type_node' have a penalty\n",
    "merged_df2.loc[merged_df2['balance_type'] == 'balance_type_none', 'node_slack_penalty'] = ''\n",
    "\n",
    "# Add the information into the df_nodes\n",
    "df_nodes['node_slack_penalty'] = merged_df2['node_slack_penalty']\n",
    "\n",
    "# Show table head for control\n",
    "df_nodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b527344-851f-47c0-a12e-72942d4176eb",
   "metadata": {},
   "source": [
    "### Relationships:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ea16b2-0e98-4ca2-b6ec-aa840ac09d20",
   "metadata": {},
   "source": [
    "#### Object__from/to_node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4c5f93-8fa4-4d88-83f9-767f83ff7ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### UNITS ###\n",
    "# Add capacities or inv_limits if not.NA from chosen values\n",
    "df_unit_capacity_data = unit_capacity_relations(df_model_units_relations, capacities_existing_params, investment_limit_params, powers_capacities)\n",
    "\n",
    "# Retrieve other relationships from database\n",
    "df_unit_relation_data = object_relationship_unit_nodes(df_model_units, df_model_units_relations)\n",
    "\n",
    "# Combine into one dataframe\n",
    "df_unit_relation_parameter_data = pd.concat([df_unit_capacity_data, df_unit_relation_data], ignore_index= True)\n",
    "\n",
    "# Add Alternative name\n",
    "df_unit_relation_parameter_data['Alternative'] = run_name\n",
    "\n",
    "# Show table head for control\n",
    "df_unit_relation_parameter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a07df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add capacities if not already given for minimum_operating_point\n",
    "df_units_min_op_point = df_unit_relation_parameter_data[df_unit_relation_parameter_data['Parameter'].str.contains('minimum_operating_point')]\n",
    "df_nodes_capacity = df_unit_relation_parameter_data[df_unit_relation_parameter_data['Parameter'].str.contains('unit_capacity')]\n",
    "\n",
    "nodes_capacity = df_nodes_capacity['Node'].tolist()\n",
    "\n",
    "rows_to_add = []\n",
    "\n",
    "# Iterate over rows that have minimum_operating_points\n",
    "for index, row in df_units_min_op_point.iterrows():\n",
    "        parameter = row['Parameter']\n",
    "        unit = row['Object_name']\n",
    "        node = row['Node']\n",
    "        relationship = row['Relationship_class_name']\n",
    "        \n",
    "        # Add to list\n",
    "        if node not in nodes_capacity:\n",
    "            row_to_add = {'Relationship_class_name': relationship,\n",
    "                  'Object_class': 'unit',\n",
    "                  'Object_name': unit,\n",
    "                  'Node': node,\n",
    "                  'Parameter': 'unit_capacity',\n",
    "                  'Value': 100\n",
    "                  }\n",
    "            rows_to_add.append(row_to_add)\n",
    "\n",
    "# Add to dataframe\n",
    "if rows_to_add:\n",
    "    df_rows_to_add = pd.DataFrame(rows_to_add)\n",
    "    df_unit_relation_parameter_data = pd.concat([df_unit_relation_parameter_data, df_rows_to_add], ignore_index=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e6058a3-6508-4218-b673-c30c1c988d0f",
   "metadata": {},
   "source": [
    "'''\n",
    "This is not necessary currently. Until we do not use power flows for running the unit on standby, we do not need this. \n",
    "standby cost are represented by unit_on_cost. Thus, if the unit is on there is some cost. \n",
    "# Add additional electricity connections if not already existent\n",
    "\n",
    "units = df_model_units.iloc[:, 0].tolist()\n",
    "length = len(units)\n",
    "data = {\n",
    "    \"Relationship_class_name\": [\"unit__from_node\"] * length,\n",
    "    \"Object_class\": [\"unit\"] * length,\n",
    "    \"Object_name\": units,\n",
    "    \"Node\": [\"power\"] * length\n",
    "}\n",
    "df_electricity = pd.DataFrame(data)\n",
    "\n",
    "units_with_from_node = df_unit_relation_parameter_data[df_unit_relation_parameter_data['Relationship_class_name'].str.contains('unit__from_node')]\n",
    "valid_object_names = units_with_from_node['Object_name']\n",
    "df_electricity_filtered = df_electricity[~df_electricity['Object_name'].isin(valid_object_names)]\n",
    "#remove the solar power plant as should not have power input\n",
    "#TODO: improve to all RES used in the case\n",
    "df_electricity_filtered = df_electricity_filtered[df_electricity_filtered['Object_name'] != 'solar_plant']\n",
    "\n",
    "merged_df = pd.merge(df_electricity_filtered, units_with_from_node, on=['Relationship_class_name', 'Object_class', 'Object_name', 'Node'], how='left', indicator=True)\n",
    "df_electricity_nodes = merged_df[merged_df['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "\n",
    "df_unit_relation_parameter_data = pd.concat([df_unit_relation_parameter_data, df_electricity_nodes], ignore_index=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92030e0-f252-49f5-b58c-defa9a0da163",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONNECTIONS ###\n",
    "df_connection_relation_parameter_data = object_relationship_connection_nodes(df_model_connections)\n",
    "\n",
    "# Add Alternatives Name\n",
    "df_connection_relation_parameter_data['Alternative'] = run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d8c41c-8fb4-4a60-883b-94b70f5ace80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined DataFrame:\n",
    "df_object__node = pd.concat([df_unit_relation_parameter_data, df_connection_relation_parameter_data])\n",
    "df_object__node = df_object__node.reset_index(drop=True)\n",
    "\n",
    "# Show df head for control\n",
    "df_object__node.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e1391f-d8cd-45dd-9867-7aefa0327740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for the definition of the object_node relationships\n",
    "df_object__node_definitions = pd.DataFrame(df_object__node[['Relationship_class_name', 'Object_class', 'Object_name', 'Node']])\n",
    "df_object__node_definitions = df_object__node_definitions.drop_duplicates()\n",
    "df_object__node_definitions = df_object__node_definitions.reset_index(drop=True)\n",
    "\n",
    "# Drop rows where no parameters for the relationship are defined (column has missing values (NaN or None))\n",
    "drop_no_value_column = 'Parameter'\n",
    "df_object__node_values = df_object__node[df_object__node[drop_no_value_column] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ad3726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add rest of to/from_node relations\n",
    "rows_to_add = []\n",
    "# Add from\n",
    "for index, row in df_model_units_relations.iterrows():\n",
    "    for i in range(1, 5):\n",
    "        input_col = f'Input{i}'\n",
    "        input_value = row[input_col]\n",
    "        \n",
    "        if pd.notna(input_value):\n",
    "            row_to_add = {'Relationship_class_name': 'unit__from_node',\n",
    "                          'Object_class': 'unit',\n",
    "                          'Object_name': row['Unit'],\n",
    "                          'Node': input_value\n",
    "                         }\n",
    "            if not any((df_object__node_definitions == pd.Series(row_to_add)).all(axis=1)):\n",
    "                rows_to_add.append(row_to_add)\n",
    "\n",
    "# Add to\n",
    "for index, row in df_model_units_relations.iterrows():\n",
    "    for i in range(1, 5):\n",
    "        output_col = f'Output{i}'\n",
    "        output_value = row[output_col]\n",
    "        \n",
    "        if pd.notna(output_value):\n",
    "            row_to_add = {'Relationship_class_name': 'unit__to_node',\n",
    "                          'Object_class': 'unit',\n",
    "                          'Object_name': row['Unit'],\n",
    "                          'Node': output_value\n",
    "                         }\n",
    "            if not any((df_object__node_definitions == pd.Series(row_to_add)).all(axis=1)):\n",
    "                rows_to_add.append(row_to_add)\n",
    "\n",
    "df_object__node_definitions = pd.concat([df_object__node_definitions, pd.DataFrame(rows_to_add)], ignore_index=True)\n",
    "\n",
    "# Show df head for control\n",
    "df_object__node_definitions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebeb807-b807-4882-b8cd-67a174c8edbd",
   "metadata": {},
   "source": [
    "#### Object__node_node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd5a160-3f18-4656-82ee-484e7e703cc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### UNITS ###\n",
    "# Search for fix ratio relations\n",
    "values_in_in_units = []\n",
    "values_in_out_units = []\n",
    "values_out_out_units = []\n",
    "for index, row in df_model_units_relations.iterrows():\n",
    "    unit = row['Unit']\n",
    "    in1 = row['Input1']\n",
    "    in2_name = f'Input{i}'\n",
    "    for i in range(2, 5):\n",
    "        inX_col = f'Input{i}'\n",
    "        inX_value = row[inX_col]\n",
    "        if not pd.isnull(row[f'Relation_In1_In{i}']):\n",
    "            value_in_in = {'Relationship': 'unit__node__node',\n",
    "                           'Object_class': 'unit',\n",
    "                           'Object_name': unit,\n",
    "                           'Node1': in1,\n",
    "                           'Node2': inX_value,\n",
    "                           'Parameter': 'fix_ratio_in_in_unit_flow',\n",
    "                           'Value': row[f'Relation_In1_In{i}'],\n",
    "                           'Alternative': run_name\n",
    "                          }\n",
    "            values_in_in_units.append(value_in_in)\n",
    "        \n",
    "        if not pd.isnull(row[f'Relation_Out1_Out{i}']):\n",
    "            value_out_out = {'Relationship': 'unit__node__node',\n",
    "                           'Object_class': 'unit',\n",
    "                           'Object_name': unit,\n",
    "                           'Node1': in1,\n",
    "                           'Node2': inX_value,\n",
    "                           'Parameter': 'fix_ratio_out_out_unit_flow',\n",
    "                           'Value': row[f'Relation_Out1_Out{i}'],\n",
    "                           'Alternative': run_name\n",
    "                          }\n",
    "            values_out_out_units.append(value_out_out)\n",
    "    if not pd.isnull(row[f'Relation_In1_Out1']):\n",
    "        value_in_out = {'Relationship': 'unit__node__node',\n",
    "                        'Object_class': 'unit',\n",
    "                        'Object_name': unit,\n",
    "                        'Node1': in1,\n",
    "                        'Node2': inX_value,\n",
    "                        'Parameter': 'fix_ratio_in_out_unit_flow',\n",
    "                        'Value': row[f'Relation_In1_Out1'],\n",
    "                           'Alternative': run_name\n",
    "                       }\n",
    "        values_in_out_units.append(value_in_out)\n",
    "\n",
    "#Combine into one df\n",
    "df_unit_node_node = pd.concat([pd.DataFrame(values_in_in_units), pd.DataFrame(values_in_out_units), pd.DataFrame(values_out_out_units)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673b014e-d173-4b0f-83cd-e18b0d15cef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONNECTIONS ###\n",
    "columns_In_In_Connection = ['Connection', 'Input1', 'Input2']\n",
    "columns_In_Out_Connection =  ['Connection', 'Input1', 'Output1']\n",
    "columns_Out_Out_Connection = ['Connection', 'Output1', 'Output2']\n",
    "columns_Out_In_Connection = ['Connection', 'Output1', 'Input1']\n",
    "\n",
    "values_in_in_connections = [('connection__node__node', 'connection', row[columns_In_In_Connection[0]], \n",
    "                             row[columns_In_In_Connection[1]], row[columns_In_In_Connection[2]], \n",
    "                             'fix_ratio_in_in_connection_flow', row['Relation_In_In'], run_name) \n",
    "                            if not pd.isnull(row[columns_In_In_Connection]).any() else (np.nan, np.nan, None) for _, row in df_model_connections.iterrows() \n",
    "                            if not pd.isnull(row[columns_In_In_Connection]).any()]\n",
    "# Comment: connections_in_out are no longer supported by Spine, therefore the values get inverted\n",
    "values_in_out_connections = [('connection__node__node', 'connection', row[columns_In_Out_Connection[0]], \n",
    "                              row[columns_In_Out_Connection[1]], row[columns_In_Out_Connection[2]], \n",
    "                              'fix_ratio_out_in_connection_flow', row['Relation_In_Out'], run_name) \n",
    "                             if not pd.isnull(row[columns_In_Out_Connection]).any() else (np.nan, np.nan, None) for _, row in df_model_connections.iterrows() \n",
    "                             if not pd.isnull(row[columns_In_Out_Connection]).any()]\n",
    "values_out_out_connections = [('connection__node__node', 'connection', row[columns_Out_Out_Connection[0]], \n",
    "                               row[columns_Out_Out_Connection[1]], row[columns_Out_Out_Connection[2]], \n",
    "                               'fix_ratio_out_out_connection_flow', row['Relation_Out_Out'], run_name) \n",
    "                              if not pd.isnull(row[columns_Out_Out_Connection]).any() else (np.nan, np.nan, None) for _, row in df_model_connections.iterrows() \n",
    "                              if not pd.isnull(row[columns_Out_Out_Connection]).any()]\n",
    "values_out_in_connections = [('connection__node__node', 'connection', row[columns_Out_In_Connection[0]], \n",
    "                               row[columns_Out_In_Connection[1]], row[columns_Out_In_Connection[2]], \n",
    "                               'fix_ratio_out_in_connection_flow', row['Relation_Out_In'], run_name) \n",
    "                              if not pd.isnull(row[columns_Out_In_Connection]).any() else (np.nan, np.nan, None) for _, row in df_model_connections.iterrows() \n",
    "                              if not pd.isnull(row[columns_Out_In_Connection]).any()]\n",
    "\n",
    "df_fix_ratio_in_in_connections = pd.DataFrame(values_in_in_connections, columns=['Relationship', 'Object_class', 'Object_name', 'Node1', \n",
    "                                                                                 'Node2', 'Parameter', 'Value', 'Alternative'])\n",
    "df_fix_ratio_in_out_connections = pd.DataFrame(values_in_out_connections, columns=['Relationship', 'Object_class', 'Object_name', 'Node1', \n",
    "                                                                                   'Node2', 'Parameter', 'Value', 'Alternative'])\n",
    "df_fix_ratio_out_out_connections = pd.DataFrame(values_out_out_connections, columns=['Relationship', 'Object_class', 'Object_name', 'Node1', \n",
    "                                                                                     'Node2', 'Parameter', 'Value', 'Alternative'])\n",
    "df_fix_ratio_out_in_connections = pd.DataFrame(values_out_in_connections, columns=['Relationship', 'Object_class', 'Object_name', 'Node1', \n",
    "                                                                                     'Node2', 'Parameter', 'Value', 'Alternative'])\n",
    "\n",
    "# Create Object_node_node\n",
    "df_object_node_node = pd.concat([df_unit_node_node, \n",
    "                                df_fix_ratio_in_in_connections, df_fix_ratio_in_out_connections, \n",
    "                                df_fix_ratio_out_out_connections, df_fix_ratio_out_in_connections])\n",
    "df_object_node_node = df_object_node_node.reset_index(drop=True)\n",
    "\n",
    "df_object_node_node = df_object_node_node.dropna(subset=['Value'])\n",
    "\n",
    "# Show table head for control\n",
    "df_object_node_node.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1388dfc3-fb27-4571-ba84-8593ea8413df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For storages, the out_in_connetion is set in both directions\n",
    "\n",
    "# Step 1: Check if the 'Object_name' column contains the word 'storage'\n",
    "storage_condition = df_object_node_node['Object_name'].str.contains('storage', case=False)\n",
    "\n",
    "# Step 2: Check if the 'Parameter' column starts with 'fix_ratio_'\n",
    "parameter_condition = df_object_node_node['Parameter'].str.startswith('fix_ratio_')\n",
    "\n",
    "# Step 3: Combine both conditions\n",
    "combined_condition = storage_condition & parameter_condition\n",
    "\n",
    "# Step 2: Update the 'Parameter' column for rows that meet the condition\n",
    "df_object_node_node.loc[storage_condition, 'Parameter'] = 'fix_ratio_out_in_connection_flow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68543d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if second node is necessary for demand\n",
    "for index, row in df_model_units.iterrows():\n",
    "    df_definition, df_nodes, df_connections, df_object__node_values, df_object_node_node = check_demand_node(\n",
    "        row, df_model_units_relations, temporal_block, resolution_to_block, df_definition, df_nodes, df_connections, \n",
    "        df_object__node_definitions, df_object__node_values, df_object_node_node, run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56d2046",
   "metadata": {},
   "source": [
    "#### Investments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0cccc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize df\n",
    "df_units_inv_parameters = pd.DataFrame(columns=['Object_name', 'unit_investment_variable_type', \n",
    "                                                'initial_units_invested_available', 'number_of_units',\n",
    "                                                'candidate_units', 'unit_investment_cost',\n",
    "                                                'unit_investment_tech_lifetime', 'unit_investment_econ_lifetime',\n",
    "                                                'Alternative'])\n",
    "\n",
    "# Choose correct column for investment costs based on chosen year\n",
    "year_columns = {int(col.split()[-1]): col for col in df_investment_costs_raw.columns if 'Value' in col}\n",
    "closest_year = min(year_columns.keys(), key=lambda x: abs(x - year))\n",
    "selected_column = year_columns[closest_year]\n",
    "\n",
    "# Add values if investment is selected\n",
    "if candidate_nonzero:\n",
    "    rows_to_add = []\n",
    "    \n",
    "    for index, row in df_model_units.iterrows():\n",
    "        object_type = row['Object_type']\n",
    "        object_type_inv = row['Object_type']\n",
    "        if object_type in ['PEM_Electrolyzer', 'AEC_Electrolyzer', 'SOEC_Electrolyzer']:\n",
    "            object_type = 'Electrolyzer'\n",
    "            \n",
    "        # Scaling by capacity\n",
    "        pattern_cap = re.compile(rf\"capacity_{object_type}\")\n",
    "        pattern_inv = re.compile(rf\"inv_limit_{object_type}\")\n",
    "        matching_vars_cap = [key for key in capacities_existing_params if pattern_cap.match(key)]\n",
    "        matching_vars_inv = [key for key in investment_limit_params if pattern_inv.match(key)]\n",
    "        \n",
    "        if matching_vars_cap:\n",
    "            capacity_name = matching_vars_cap[0]\n",
    "            capacity = capacities_existing_params[capacity_name]\n",
    "        else:\n",
    "            capacity = 0\n",
    "        if matching_vars_inv:\n",
    "            investment_limit_name = matching_vars_inv[0]\n",
    "            investment_limit = investment_limit_params[investment_limit_name]\n",
    "        else:\n",
    "            investment_limit = 100\n",
    "        \n",
    "        # Default investment cost\n",
    "        matching_row = df_investment_costs_raw[df_investment_costs_raw['Object_type'] == object_type_inv]\n",
    "        if not matching_row.empty:\n",
    "            # Extract the investment cost and lifetime from matching_row\n",
    "            investment_cost = matching_row[selected_column].values[0]\n",
    "            lifetime_str = matching_row['Lifetime'].values[0]\n",
    "            \n",
    "            # Convert the lifetime to days\n",
    "            lifetime = convert_to_days(lifetime_str, year)\n",
    "        else:\n",
    "            investment_cost = 0  # Default investment cost if no match is found\n",
    "            lifetime = '10950D'  # Default lifetime if no match is found\n",
    "        \n",
    "        default_value = investment_cost * investment_limit\n",
    "        \n",
    "        # Externally chosen investment cost        \n",
    "        pattern = re.compile(rf\"inv_cost_.*{object_type}\")\n",
    "        matching_vars = [key for key in investment_cost_params if pattern.match(key)]\n",
    "        \n",
    "        if matching_vars:\n",
    "            variable_name = matching_vars[0]\n",
    "            variable_value = investment_cost_params[variable_name]\n",
    "            \n",
    "            if variable_value is not None:\n",
    "                if investment_limit:\n",
    "                    unit_investment_cost = variable_value * investment_limit\n",
    "                else:\n",
    "                    unit_investment_cost = variable_value * capacity\n",
    "            else:\n",
    "                unit_investment_cost = default_value\n",
    "        else:\n",
    "            # If no external costs, use the default cost\n",
    "            unit_investment_cost = default_value\n",
    "        \n",
    "        # Calculate  initial units invested available + number of units\n",
    "        if investment_res:\n",
    "            if investment_limit and capacity:\n",
    "                if capacity:\n",
    "                    value = capacity / investment_limit\n",
    "            else:\n",
    "                value = 0\n",
    "        else:\n",
    "            if object_type in {'PV_plant', 'Wind_onshore', 'Wind_offshore'}:\n",
    "                value = 1\n",
    "            else:\n",
    "                if investment_limit and capacity:\n",
    "                    if capacity:\n",
    "                        value = capacity / investment_limit\n",
    "                else:\n",
    "                    value = 0\n",
    "\n",
    "        # Add the results for this row to df_units_inv_parameters\n",
    "        row_to_add = {'Object_name': row['Unit'],\n",
    "                      'unit_investment_variable_type': 'unit_investment_variable_type_continuous',\n",
    "                      'initial_units_invested_available': value,\n",
    "                      'number_of_units': 0,\n",
    "                      'candidate_units': 1, # 1 = investment limit\n",
    "                      'unit_investment_cost': unit_investment_cost,\n",
    "                      'unit_investment_tech_lifetime': lifetime,\n",
    "                      'unit_investment_econ_lifetime': lifetime,  # Same as tech lifetime\n",
    "                      'Alternative': run_name\n",
    "                     }\n",
    "        \n",
    "        rows_to_add.append(row_to_add)\n",
    "        \n",
    "    df_units_inv_parameters = pd.concat([df_units_inv_parameters, pd.DataFrame(rows_to_add)], ignore_index=True)\n",
    "    \n",
    "    # Scale investment costs to lifetime\n",
    "    df_units_inv_parameters = scale_costs(df_units_inv_parameters, start_date, end_date, 'unit')\n",
    "    \n",
    "    # Scale investment costs to roll forward if used\n",
    "    if roll_forward_use:\n",
    "        df_units_inv_parameters['unit_investment_cost'] = df_units_inv_parameters['unit_investment_cost'] / int(num_slices)\n",
    "    \n",
    "# Show table head for control\n",
    "df_units_inv_parameters.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df60b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize df\n",
    "df_connections_inv_parameters = pd.DataFrame(columns=['Object_name', 'connection_type', 'connection_investment_variable_type',\n",
    "                                                      'initial_connections_invested_available', 'number_of_connections',\n",
    "                                                      'candidate_connections', 'connection_investment_cost',\n",
    "                                                      'connection_investment_tech_lifetime', 'connection_investment_econ_lifetime', \n",
    "                                                      'Alternative'])\n",
    "\n",
    "# Add values if investment is selected\n",
    "if candidate_nonzero:\n",
    "    rows_to_add = []\n",
    "    \n",
    "    for index, row in df_model_connections.iterrows():\n",
    "        object_type = row['Object_type']\n",
    "        \n",
    "        matching_row = df_investment_costs_raw[df_investment_costs_raw['Object_type'] == object_type]\n",
    "        if not matching_row.empty:\n",
    "            # Extract the investment cost and lifetime from matching_row\n",
    "            investment_cost_default = matching_row[selected_column].values[0]\n",
    "        else:\n",
    "            investment_cost_default = 0  # Default investment cost if no match is found\n",
    "        \n",
    "        connection_type = row['Connection_type']\n",
    "        lifetime = row['connection_investment_tech_lifetime']\n",
    "        lifetime_str = row['connection_investment_tech_lifetime']\n",
    "        # Convert the lifetime to days\n",
    "        lifetime = convert_to_days(lifetime_str, year)\n",
    "        \n",
    "        # Add the results for this row to df_units_inv_parameters\n",
    "        row_to_add = {'Object_name': row['Connection'],\n",
    "                      'connection_type': connection_type,\n",
    "                      'connection_investment_variable_type': 'connection_investment_variable_type_continuous',\n",
    "                      'initial_connections_invested_available': row['initial_connections_invested_available'],\n",
    "                      'number_of_connections': 0,\n",
    "                      'candidate_connections': 1,\n",
    "                      'connection_investment_cost': investment_cost_default,\n",
    "                      'connection_investment_tech_lifetime': lifetime,\n",
    "                      'connection_investment_econ_lifetime': lifetime,  # Same as tech lifetime\n",
    "                      'Alternative': run_name\n",
    "        }\n",
    "        \n",
    "        rows_to_add.append(row_to_add)\n",
    "        \n",
    "    df_connections_inv_parameters = pd.concat([df_connections_inv_parameters, pd.DataFrame(rows_to_add)], ignore_index=True)\n",
    "    \n",
    "    # Scale investment costs to lifetime\n",
    "    df_connections_inv_parameters = scale_costs(df_connections_inv_parameters, start_date, end_date, 'connection')\n",
    "    \n",
    "    # Scale investment costs to roll forward if used\n",
    "    if roll_forward_use:\n",
    "        df_connections_inv_parameters['connection_investment_cost'] = df_connections_inv_parameters['connection_investment_cost'] / int(num_slices)\n",
    "    \n",
    "# Show table head for control\n",
    "df_connections_inv_parameters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a188ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize df\n",
    "df_storages_inv_parameters = pd.DataFrame(columns=['Object_name', 'storage_investment_variable_type',\n",
    "                                                   'initial_storages_invested', 'number_of_storages',\n",
    "                                                   'candidate_storages', 'storage_investment_cost',\n",
    "                                                   'storage_investment_tech_lifetime', 'storage_investment_econ_lifetime', \n",
    "                                                   'Alternative'])\n",
    "\n",
    "# Add values if investment is selected\n",
    "if candidate_nonzero:\n",
    "    rows_to_add = []\n",
    "    \n",
    "    for index, row in df_model_storages.iterrows():\n",
    "        object_type = row['Object_type']\n",
    "        \n",
    "        # Investment cost\n",
    "        #default\n",
    "        matching_row = df_investment_costs_raw[df_investment_costs_raw['Object_type'] == object_type]\n",
    "        \n",
    "        #chosen\n",
    "        pattern_cost = re.compile(rf\"inv_cost_.*{object_type}\")\n",
    "        matching_vars_cost = [key for key in investment_cost_params if pattern_cost.match(key)]\n",
    "        \n",
    "        if pd.notna(matching_vars_cost):\n",
    "            variable_name = matching_vars_cost[0]\n",
    "            variable_value = investment_cost_params[variable_name]\n",
    "            \n",
    "            if pd.notna(variable_value):\n",
    "                investment_cost = variable_value\n",
    "            else: \n",
    "                if not matching_row.empty:\n",
    "                    investment_cost = matching_row[selected_column].values[0]\n",
    "                else: \n",
    "                    investment_cost = 0\n",
    "        else:\n",
    "            if not matching_row.empty:\n",
    "                investment_cost = matching_row[selected_column].values[0]\n",
    "            else:\n",
    "                investment_cost = 0  # Default investment cost if no match is found\n",
    "        \n",
    "        # Scaling by inv_limit\n",
    "        pattern_inv_limit = re.compile(rf\"inv_limit_{object_type}\")\n",
    "        matching_vars_inv_limit = [key for key in investment_limit_params if pattern_inv_limit.match(key)]\n",
    "        \n",
    "        if pd.notna(matching_vars_inv_limit):\n",
    "            inv_limit_name = matching_vars_inv_limit[0]\n",
    "            inv_limit = investment_limit_params[inv_limit_name]\n",
    "            \n",
    "            if math.isnan(inv_limit):\n",
    "                inv_limit = row['node_state_cap'] if not pd.isna(row['node_state_cap']) else 0\n",
    "        else:\n",
    "            inv_limit = row['node_state_cap'] if not pd.isna(row['node_state_cap']) else 0\n",
    "        \n",
    "        scaled_investment_cost = investment_cost * inv_limit\n",
    "                \n",
    "        # Lifetime \n",
    "        if not matching_row.empty:\n",
    "            lifetime_str = matching_row['Lifetime'].values[0]\n",
    "            lifetime = convert_to_days(lifetime_str, year) # Convert the lifetime to days\n",
    "        else:\n",
    "            lifetime = '10950D'  # Default lifetime if no match is found\n",
    "        \n",
    "        # Add the results for this row to df_units_inv_parameters\n",
    "        row_to_add = {'Object_name': row['Storage'],\n",
    "                      'storage_investment_variable_type': 'storage_investment_variable_type_continuous',\n",
    "                      'initial_storages_invested': row['initial_storages_invested'],\n",
    "                      'number_of_storages': 0,\n",
    "                      'candidate_storages': 1,\n",
    "                      'storage_investment_cost': scaled_investment_cost,\n",
    "                      'storage_investment_tech_lifetime': lifetime,\n",
    "                      'storage_investment_econ_lifetime': lifetime,  # Same as tech lifetime\n",
    "                      'Alternative': run_name\n",
    "        }\n",
    "        \n",
    "        rows_to_add.append(row_to_add)\n",
    "        \n",
    "    df_storages_inv_parameters = pd.concat([df_storages_inv_parameters, pd.DataFrame(rows_to_add)], ignore_index=True)\n",
    "    \n",
    "    # Scale investment costs from lifetime\n",
    "    df_storages_inv_parameters = scale_costs(df_storages_inv_parameters, start_date, end_date, 'storage')\n",
    "    \n",
    "    # Scale investment costs to roll forward if used\n",
    "    if roll_forward_use:\n",
    "        df_storages_inv_parameters['storage_investment_cost'] = df_storages_inv_parameters['storage_investment_cost'] / int(num_slices)\n",
    "    \n",
    "# Show data head for control\n",
    "df_storages_inv_parameters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a065003-5983-4a8f-9914-c2b0e35feac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the node_state_cap if an investment_limit exists as the parameter limits the capacity of a storage\n",
    "# for object in df_storages_inv_parameters:\n",
    "# look for corresponding object name in df_nodes and change value in column node_state_cap to inv_limit\n",
    "for index, row in df_model_storages.iterrows():\n",
    "    object_name = row['Storage']\n",
    "    object_type = row['Object_type']\n",
    "    \n",
    "    pattern_inv_limit = re.compile(rf\"inv_limit_{object_type}\")\n",
    "    matching_vars_inv_limit = [key for key in investment_limit_params if pattern_inv_limit.match(key)]\n",
    "    \n",
    "    if matching_vars_inv_limit:\n",
    "        inv_limit_name = matching_vars_inv_limit[0]\n",
    "        inv_limit = investment_limit_params[inv_limit_name]\n",
    "        df_nodes.loc[df_nodes['Object_name'] == object_name, 'node_state_cap'] = inv_limit\n",
    "\n",
    "# Show data head for control\n",
    "df_nodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55192bdb",
   "metadata": {},
   "source": [
    "### Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b971c4",
   "metadata": {},
   "source": [
    "#### Model relations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf465d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing definition of model components\n",
    "column_names_model_components = {'Object_class_name':['model','temporal_block','stochastic_scenario', 'stochastic_structure', 'report'],\n",
    "                      'Object_name': [model_name, temporal_block, stochastic_scenario, stochastic_structure, report_name]}\n",
    "df_model_components = pd.DataFrame(column_names_model_components, index=None)\n",
    "\n",
    "# Adding a default investment temporal block if investment = true \n",
    "if candidate_nonzero:\n",
    "    df_model_components.loc[len(df_model_components.index)] = ['temporal_block', 'Default_Investment_period']\n",
    "\n",
    "# Outputs:\n",
    "reports_list = list(reports)\n",
    "df_outputs = pd.DataFrame({\n",
    "    'Object_class_name': ['output']*len(reports_list),\n",
    "    'Object_name': reports_list\n",
    "})\n",
    "df_model_components = pd.concat([df_model_components, df_outputs], axis=0)\n",
    "df_model_components = df_model_components.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c335dd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add reports chosen in main\n",
    "column_names_model_structure = {'Object_class_name':['model','temporal_block','stochastic_scenario', 'stochastic_structure', 'report'],\n",
    "                      'Object_name': [model_name, temporal_block, stochastic_scenario, stochastic_structure, report_name]}\n",
    "df_reports = pd.DataFrame({\n",
    "    'Relationship_class_name': ['report__output']*len(reports_list),\n",
    "    'Object_class_name_1': ['report']*len(reports_list),\n",
    "    'Object_class_name_2': ['output']*len(reports_list),\n",
    "    'Object_name_1': [report_name]*len(reports_list),\n",
    "    'Object_name_2': reports_list\n",
    "})\n",
    "\n",
    "# Add everything else\n",
    "df_model_struc = pd.DataFrame({\n",
    "    'Relationship_class_name': ['model__temporal_block','model__default_temporal_block', \n",
    "                                'model__stochastic_structure', 'model__default_stochastic_structure',\n",
    "                                'stochastic_structure__stochastic_scenario', 'model__report'],\n",
    "    'Object_class_name_1': ['model','model','model','model','stochastic_structure','model'],\n",
    "    'Object_class_name_2': ['temporal_block', 'temporal_block','stochastic_structure','stochastic_structure', 'stochastic_scenario','report'],\n",
    "    'Object_name_1': [model_name, model_name,model_name,model_name,stochastic_structure, model_name],\n",
    "    'Object_name_2': [temporal_block, temporal_block, stochastic_structure, stochastic_structure, stochastic_scenario, report_name]\n",
    "})\n",
    "df_model_relations = pd.concat([df_model_struc, df_reports], axis=0)\n",
    "df_model_relations = df_model_relations.reset_index(drop=True)\n",
    "\n",
    "# Defining default investment temporal block and stochastic structure if investment = true\n",
    "if candidate_nonzero:\n",
    "    df_model_relations.loc[len(df_model_relations.index)] = ['model__default_investment_temporal_block', 'model', 'temporal_block', model_name, 'Default_Investment_period']\n",
    "    df_model_relations.loc[len(df_model_relations.index)] = ['model__default_investment_stochastic_structure', 'model', 'stochastic_structure', model_name, stochastic_structure]\n",
    "\n",
    "# Show table head for control\n",
    "df_model_relations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892b88dc-3a5b-41c5-b184-64593c28c72c",
   "metadata": {},
   "source": [
    "#### Start, end, resolution :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3960d14b-394b-4599-9e1e-8831aa59e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create values for model structure\n",
    "column_names_model = {'Object_class_name':['model','model','temporal_block', 'model'],\n",
    "                      'Object_name': [model_name, model_name, temporal_block, model_name],\n",
    "                      'Parameter':['model_start','model_end','resolution', 'db_mip_solver_options'],\n",
    "                      'Value': ['{\"type\": \"date_time\", \"data\": \"'+df_time.iloc[0]['DateTime']+'\"}',\n",
    "                                '{\"type\": \"date_time\", \"data\": \"'+df_time.iloc[-1]['DateTime']+'\"}',\n",
    "                                '{\"type\":\"duration\", \"data\": \"'+frequency+'\"}',\n",
    "                                '{\"type\": \"map\", \"data\": [[\"HiGHS.jl\", {\"data\": [[\"presolve\", \"on\"], [\"mip_rel_gap\", 0.0000000001], [\"threads\", 0.0], [\"time_limit\", 3000.01]], \"type\": \"map\", \"index_type\": \"str\"}], [\"Cbc.jl\", {\"data\": [[\"ratioGap\", 0.01], [\"logLevel\", 0.0]], \"type\": \"map\", \"index_type\": \"str\"}], [\"CPLEX.jl\", {\"data\": [[\"CPX_PARAM_EPGAP\", 0.01]], \"type\": \"map\", \"index_type\": \"str\"}]], \"index_type\": \"str\"}'\n",
    "                               ],\n",
    "                      'Alternative': [run_name, run_name, run_name, run_name]\n",
    "                     }\n",
    "df_model = pd.DataFrame(column_names_model, index=None)\n",
    "\n",
    "# Add investment temporal blocks to model data frame in days if investment = true\n",
    "if candidate_nonzero:\n",
    "    df_model.loc[len(df_model.index)] = ['temporal_block', 'Default_Investment_period', 'resolution', '{\"type\":\"duration\", \"data\": \"'+convert_to_days(investment_period_default, year)+'\"}', run_name]\n",
    "\n",
    "# Show how table head for control\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b24d678-4167-4521-a6ab-aae160dc13ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add information in case roll forward is used\n",
    "if roll_forward_use:\n",
    "    if temporal_block == 'hourly':\n",
    "        unit = 'h'\n",
    "    elif temporal_block == 'daily':\n",
    "        unit = 'D'\n",
    "    elif temporal_block == 'monthly':\n",
    "        unit = 'M'\n",
    "    else:\n",
    "        unit = ''\n",
    "        print(\"\\033[91mWARNING:\\033[0m Duration not defined!!!\")\n",
    "    # Add the new row to the df\n",
    "    roll_forward_size_with_unit = str(roll_forward_size) + unit\n",
    "    roll_forward_row = {'Object_class_name': 'model', \n",
    "                        'Object_name': model_name, \n",
    "                        'Parameter': 'roll_forward',\n",
    "                        'Value': '{\"type\": \"duration\", \"data\": \"' + roll_forward_size_with_unit +'\"}',\n",
    "                        'Alternative': run_name\n",
    "                       }\n",
    "    # Add new row to df_model DataFrame\n",
    "    df_model.loc[len(df_model)] = roll_forward_row\n",
    "    '''This part is not finalized and active as it is not working in the model (the end of the optimization window is a problem). \n",
    "    #add information for overlap of the roll forward period. Thus, the model optimizes a bit longer than just the roll_forward\n",
    "    block_start_row = {'Object_class_name': 'model', \n",
    "                        'Object_name': model_name, \n",
    "                        'Parameter': 'block_start',\n",
    "                        'Value': '{\"type\": \"duration\", \"data\": \"' + str(0) + unit +'\"}',\n",
    "                        'Alternative': run_name,\n",
    "                       }\n",
    "    # Add new row to df_model DataFrame\n",
    "    df_model.loc[len(df_model)] = block_start_row\n",
    "\n",
    "    #add information for overlap of the roll forward period. Thus, the model optimizes a bit longer than just the roll_forward\n",
    "    block_end_row = {'Object_class_name': 'model', \n",
    "                        'Object_name': model_name, \n",
    "                        'Parameter': 'block_end',\n",
    "                        'Value': '{\"type\": \"duration\", \"data\": \"' + str(roll_forward_size + 48) + unit +'\"}',\n",
    "                        'Alternative': run_name\n",
    "                       }\n",
    "     # Add new row to df_model DataFrame\n",
    "    df_model.loc[len(df_model)] = block_end_row'''\n",
    "\n",
    "# Show table head for control\n",
    "df_model.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14842492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal resolution for demand\n",
    "# Add temporal block to definition, if necessary\n",
    "check_temporal_block(df_model_units, df_model_components)\n",
    "\n",
    "# Add temporal relation to respective node\n",
    "df_temporal_relations = pd.DataFrame(columns = ['Relationship_class_name', 'Node', 'Temporal_block'], index=None)\n",
    "create_temporal_block_relationships(df_model_units, df_model_units_relations, df_model_relations, model_name, df_definition, df_temporal_relations)\n",
    "\n",
    "# Add values to temporal block\n",
    "create_temporal_block_input(df_model_units, df_model, run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36be54c5-1d66-4841-a002-30a94697b84e",
   "metadata": {},
   "source": [
    "### Variable Efficiency Units:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17268ecd-53bc-49ec-b6b8-6132ce181b2f",
   "metadata": {},
   "source": [
    "#### Calculate adjusted efficiency for each unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d2d2fd-57ee-48ed-866a-7093e5dfefb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_units_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4062a8-9dc3-490e-a541-c483bfb2e65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important as the SpineOpt implementation is piecewise but when producing at high capacity the lower segments also produce\n",
    "# This way, we avoid overestimation of production when running at higher capacities\n",
    "\n",
    "for index, row in df_model_units_relations.iterrows():\n",
    "    unit = row['Unit'].lower()\n",
    "    mean_efficiency_series = df_model_units.loc[df_model_units['Unit'] == unit, 'mean_efficiency']\n",
    "    if not mean_efficiency_series.empty:\n",
    "        mean_efficiency = mean_efficiency_series.iloc[0]  # Get the first match\n",
    "    else:\n",
    "        mean_efficiency = None  # Or any default value you want\n",
    "    \n",
    "    # If 'mean_efficiency' column has a value, check if corresponding DataFrame exists\n",
    "    if pd.notna(mean_efficiency):\n",
    "        node_capa = (df_unit_capacity_data.loc[df_unit_capacity_data['Object_name'] == unit, 'Node']).iloc[0]\n",
    "        if any(node_capa in str(row[f'Input{i}']) for i in range(1, 4)):\n",
    "            input_1 = node_capa\n",
    "            output_1 = row['Output1']\n",
    "        else:\n",
    "            input_1 = row['Input1']\n",
    "            output_1 = node_capa\n",
    "        df_name = f\"df_efficiency_{unit}\"\n",
    "        \n",
    "        if df_name in globals() and isinstance(globals()[df_name], pd.DataFrame):\n",
    "            \n",
    "            # Calculate adjusted efficiency\n",
    "            df_efficiency_adj = create_adj_efficiency(globals()[df_name], mean_efficiency, unit)\n",
    "            globals()[f\"df_efficiency_{unit}_adj\"] = df_efficiency_adj\n",
    "            \n",
    "            # Fit with operating points\n",
    "            des_segment = globals()['des_segments_' + unit]\n",
    "            df_var_efficiency, df_operating_points, segment_x_values, segment_averages, x_values, y_values = calculate_op_points(\n",
    "                unit, int(des_segment), df_efficiency_adj, input_1, output_1, run_name\n",
    "            )\n",
    "            globals()[f\"variable_efficiency_{unit}\"] = df_var_efficiency\n",
    "            globals()[f\"operating_points_{unit}\"] = df_operating_points\n",
    "            globals()[f\"segment_x_values_{unit}\"] = segment_x_values\n",
    "            globals()[f\"segment_averages_{unit}\"] = segment_averages\n",
    "            globals()[f\"x_values_{unit}\"] = x_values\n",
    "            globals()[f\"y_values_{unit}\"] = y_values\n",
    "            \n",
    "            # Create ordered_unit_flow\n",
    "            ordered_unit_flow = check_decreasing(df_var_efficiency, unit, input_1, run_name)\n",
    "            globals()[f\"ordered_unit_flow_{unit}\"] = ordered_unit_flow\n",
    "            \n",
    "        else:\n",
    "            print(f\"WARNING: No variable efficiency defined for {unit}\")\n",
    "    else:\n",
    "        # If 'mean_efficiency' column is NA, skip\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602893a4-cf5b-4f48-b38c-45e045466537",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_var_efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf17dac-b552-4369-84fc-4417439747a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the data of the original efficiency curve and the adjusted ones\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(df_efficiency_electrolyzer_adj['Power [%]'], df_efficiency_electrolyzer_adj['Efficiency [%]'], label='Eff raw', marker='o')\n",
    "plt.plot(df_efficiency_electrolyzer_adj['Power [%]'], df_efficiency_electrolyzer_adj['Efficiency_scaled [%]'], label='Eff scaled', marker='x')\n",
    "plt.plot(df_efficiency_adj['Power [%]'], df_efficiency_adj['eff_adjusted_electrolyzer'], label='Eff adjusted', marker='x', \n",
    "         linestyle='dashed', dashes=(5, 7))\n",
    "\n",
    "plt.xlabel('Power [%]')\n",
    "plt.ylabel('Efficiency [%]')\n",
    "plt.title('Efficiency vs Power')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6107fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the data points and the curve\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.scatter(df_efficiency_electrolyzer_adj['Power [%]'], df_efficiency_electrolyzer_adj['eff_adjusted_electrolyzer'], color='blue', label='Efficiency Adjusted')\n",
    "plt.plot(x_values_electrolyzer, y_values_electrolyzer, color='red', label='Fitted Efficiency Curve Electrolyzer')\n",
    "\n",
    "# Plotting segment averages\n",
    "for i, (x_start, x_end) in enumerate(segment_x_values_electrolyzer):\n",
    "    plt.plot([x_start, x_end], [segment_averages_electrolyzer[i], segment_averages_electrolyzer[i]], color='green', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Curve Fitted to Data Points')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722bb069",
   "metadata": {},
   "source": [
    "#### Add operating points to user constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38694351",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Summarize all variable efficiencies\n",
    "# Collect all data frames that start with variable_efficiency\n",
    "dfs = [value for key, value in globals().items() if key.startswith('variable_efficiency') and isinstance(value, pd.DataFrame)]\n",
    "\n",
    "# Choose the first column with the most segments\n",
    "longest_df = max(dfs, key=lambda df: df.shape[0])\n",
    "first_column = longest_df.iloc[:, 0]\n",
    "\n",
    "# Concatenate the remaining columns from all DataFrames, ensuring no duplicate \"first column\"\n",
    "remaining_columns = [df.iloc[:, 1:] for df in dfs]\n",
    "\n",
    "# Concatenate the remaining columns to the longest first column\n",
    "df_variable_efficiency = pd.concat([first_column] + remaining_columns, axis=1)\n",
    "df_variable_efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b94c1a0-c73d-45b6-b048-df9ae8c0261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relate all user_constraints to entity class name \"user_constraint\" \n",
    "\n",
    "Entity_names_duplicate = df_variable_efficiency.iloc[0,1:]\n",
    "User_constraint_entities = []\n",
    "for name in Entity_names_duplicate:\n",
    "    if name not in User_constraint_entities:\n",
    "        User_constraint_entities.append(name)\n",
    "        \n",
    "\n",
    "User_constraint_column = [\"user_constraint\"]*len(User_constraint_entities)\n",
    "\n",
    "merged_columns = {\"Entity class names\": User_constraint_column, \"Entity names\": User_constraint_entities}\n",
    "df_variable_eff_def= pd.DataFrame(merged_columns)\n",
    "df_variable_eff_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fb592c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summarize all operating points\n",
    "# Collect all data frames that start with operating_points\n",
    "dfs = [value for key, value in globals().items() if key.startswith('operating_points') and isinstance(value, pd.DataFrame)]\n",
    "\n",
    "# Choose the first column with the most segments\n",
    "longest_df = max(dfs, key=lambda df: df.shape[0])\n",
    "first_column = longest_df.iloc[:, 0]\n",
    "\n",
    "# Concatenate the remaining columns from all DataFrames, ensuring no duplicate \"first column\"\n",
    "remaining_columns = [df.iloc[:, 1:] for df in dfs]\n",
    "\n",
    "# Concatenate the remaining columns to the longest first column\n",
    "df_operating_points = pd.concat([first_column] + remaining_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899403f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summarize all ordered_unit_flow_op\n",
    "# Collect all data frames that start with ordered_unit_flow_op\n",
    "dfs = [value for key, value in globals().items() if key.startswith('ordered_unit_flow_') and isinstance(value, pd.DataFrame)]\n",
    "\n",
    "# Concatenate the remaining columns to the longest first column\n",
    "df_boolean_relations = pd.concat(dfs, ignore_index=True)\n",
    "df_boolean_relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f878c0",
   "metadata": {},
   "source": [
    "### Environment and Storages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af08c35",
   "metadata": {},
   "source": [
    "#### Time Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33534c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust renewables columns names\n",
    "df_PV_availabilityfactors_values.rename(columns={'time': 'time [UTC]', \n",
    "                                                 'local_time': 'time [' + area + ']',\n",
    "                                                 'electricity': 'unit_availability_factor'}, inplace=True)\n",
    "df_wind_availabilityfactors_values.rename(columns={'time': 'time [UTC]',\n",
    "                                                   'local_time': 'time [' + area + ']',\n",
    "                                                   'electricity_onshore': 'unit_availability_factor_onshore',\n",
    "                                                   'electricity_offshore': 'unit_availability_factor_offshore'}, inplace=True)\n",
    "# Adjust power prices\n",
    "df_powerprices_values.rename(columns={'HourUTC': 'time [UTC]', \n",
    "                                         'HourDK': 'time [' + area + ']'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b06362-af38-4e4b-ab1b-fc2f8024bee0",
   "metadata": {},
   "source": [
    "#### Renewables Availability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c49bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table headers and relations\n",
    "column_names_1 = {'DateTime '+area: ['Object Type', 'Alternative', 'Parameter']}\n",
    "\n",
    "# Add time values\n",
    "df_temp_1 = pd.DataFrame(columns=['DateTime ' + area])\n",
    "df_temp_1['DateTime '+area] = df_time\n",
    "\n",
    "# Add solar values\n",
    "if 'Solar plant' in powers:\n",
    "    column_names_1['solar_plant'] = ['unit', run_name, 'unit_availability_factor']\n",
    "    df_temp_1['solar_plant'] = df_PV_availabilityfactors_values['unit_availability_factor']\n",
    "    \n",
    "    # Fill NaNs on last day by copying previous day (only for leap years)\n",
    "    df_temp_1['solar_plant_shifted'] = df_temp_1['solar_plant'].shift(24)\n",
    "    df_temp_1['solar_plant'] = df_temp_1['solar_plant'].fillna(df_temp_1['solar_plant_shifted'])\n",
    "    df_temp_1 = df_temp_1.drop(columns=['solar_plant_shifted'])\n",
    "\n",
    "# Add onshore wind values\n",
    "if 'Wind onshore' in powers:\n",
    "    column_names_1['wind_onshore'] = ['unit', run_name, 'unit_availability_factor']\n",
    "    df_temp_1['wind_onshore'] = df_wind_availabilityfactors_values['unit_availability_factor_onshore']\n",
    "    \n",
    "    # Fill NaNs on last day by copying previous day (only for leap years)\n",
    "    df_temp_1['wind_onshore_shifted'] = df_temp_1['wind_onshore'].shift(24)\n",
    "    df_temp_1['wind_onshore'] = df_temp_1['wind_onshore'].fillna(df_temp_1['wind_onshore_shifted'])\n",
    "    df_temp_1 = df_temp_1.drop(columns=['wind_onshore_shifted'])\n",
    "\n",
    "# Add offshore wind values\n",
    "if 'Wind offshore' in powers:\n",
    "    column_names_1['wind_offshore'] = ['unit', run_name, 'unit_availability_factor']\n",
    "    df_temp_1['wind_offshore'] = df_wind_availabilityfactors_values['unit_availability_factor_offshore']\n",
    "    \n",
    "    # Fill NaNs on last day by copying previous day (only for leap years)\n",
    "    df_temp_1['wind_offshore_shifted'] = df_temp_1['wind_offshore'].shift(24)\n",
    "    df_temp_1['wind_offshore'] = df_temp_1['wind_offshore'].fillna(df_temp_1['wind_offshore_shifted'])\n",
    "    df_temp_1 = df_temp_1.drop(columns=['wind_offshore_shifted'])\n",
    "\n",
    "\n",
    "#Add chosen availability factors to one combined dataframe\n",
    "df_blank_table_1 = pd.DataFrame(column_names_1, index=None)\n",
    "df_time_series = pd.concat([df_blank_table_1, df_temp_1])\n",
    "\n",
    "# Show table head for control\n",
    "df_time_series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e74f16-846f-4ba0-b9c6-869e32be6676",
   "metadata": {},
   "source": [
    "#### Energy prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4679f8e3-8ce3-4f00-b57e-e3f1427db970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjustments of power price variance\n",
    "\n",
    "# Calculate the current mean and variance\n",
    "mean = df_powerprices_values[str(area)].mean()\n",
    "current_variance = df_powerprices_values[str(area)].var()\n",
    "\n",
    "# Define the new desired variance \n",
    "desired_variance = current_variance * power_price_variance\n",
    "\n",
    "# Calculate the scaling factor\n",
    "scaling_factor = np.sqrt(desired_variance / current_variance)\n",
    "\n",
    "# Adjust the time series to achieve the new variance\n",
    "df_powerprices_values[str(area)] = mean + (df_powerprices_values[str(area)] - mean) * scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a785d20b-2cbe-4f0b-8842-2878a3335580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create table\n",
    "column_names_2 = {'DateTime ' + area: ['Object type', 'Relationship class','Object name','Node', 'Alternative', 'Parameter name'],\n",
    "                'power_wholesale_in': ['connection','connection__from_node','pl_wholesale','power_wholesale', run_name, 'connection_flow_cost'], \n",
    "                'power_wholesale_out': ['connection','connection__to_node','pl_wholesale','power_wholesale', run_name, 'connection_flow_cost'], \n",
    "                'district_heating': ['connection','connection__to_node','pl_dh','dh', run_name, 'connection_flow_cost']}\n",
    "df_blank_table_2 = pd.DataFrame(column_names_2, index=None)\n",
    "\n",
    "# Add values\n",
    "df_temp_2 = pd.DataFrame(columns=['DateTime ' + area, 'power_wholesale_in', 'power_wholesale_out', 'district_heating'])\n",
    "\n",
    "df_temp_2['DateTime ' + area] = df_time\n",
    "df_temp_2['power_wholesale_in'] = price_level_power * df_powerprices_values[str(area)]\n",
    "df_temp_2['power_wholesale_out'] = -1 * price_level_power * df_powerprices_values[str(area)]\n",
    "\n",
    "value_dh = df_other_costs[df_other_costs.iloc[:, 0] == 'district_heating'].iloc[0, 2]\n",
    "df_temp_2['district_heating'] = -1 * value_dh * share_of_dh_price_cap\n",
    "\n",
    "df_table_2 = pd.concat([df_blank_table_2, df_temp_2], ignore_index=True)\n",
    "\n",
    "# Remove wholesale \n",
    "if 'Grid' not in powers:\n",
    "    df_table_2 = df_table_2.drop(columns=['power_wholesale_in', 'power_wholesale_out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca46fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add grid costs for consumption and production\n",
    "if 'Grid' in powers:\n",
    "    # Create table\n",
    "    column_names_3 = {'grid_costs_consumption': ['connection', 'connection__to_node', 'pl_wholesale', 'power', run_name, 'connection_flow_cost'],\n",
    "                 'grid_costs_production': ['connection', 'connection__from_node', 'pl_wholesale', 'power', run_name, 'connection_flow_cost']}\n",
    "    df_blank_table_3 = pd.DataFrame(column_names_3, index=None)\n",
    "    \n",
    "    # Add values\n",
    "    df_temp_3 = pd.DataFrame(columns=['grid_costs_consumption', 'grid_costs_production'])\n",
    "    \n",
    "    df_temp_3['grid_costs_consumption'] = [df_other_costs[df_other_costs.iloc[:, 0] == 'grid_out'].iloc[0, 2]] * len(df_time)\n",
    "    df_temp_3['grid_costs_production'] = df_other_costs[df_other_costs.iloc[:, 0] == 'grid_in'].iloc[0, 2]\n",
    "    \n",
    "    df_table_3 = pd.concat([df_blank_table_3, df_temp_3], ignore_index = True)\n",
    "    \n",
    "    # Merge all energy prices\n",
    "    df_energy_prices = pd.concat([df_table_2, df_table_3], axis = 1)\n",
    "else:\n",
    "    df_energy_prices = df_table_2\n",
    "\n",
    "# Show table head for control\n",
    "df_energy_prices.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b8b01b-e639-4dfd-afa0-191c850a02fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_costs_columns = {}\n",
    "\n",
    "for _, row in df_model_units_relations.iterrows():\n",
    "    unit = row['Unit']\n",
    "    input_node = row['Input1']\n",
    "\n",
    "    if input_node in [\"water_source\", \"co2_source\", \"o2_source\"]:\n",
    "        matching = df_other_costs[df_other_costs.iloc[:, 0] == input_node]\n",
    "        if not matching.empty:\n",
    "            value = matching.iloc[0, 2]\n",
    "            if pd.notna(value):\n",
    "                # Create the column values\n",
    "                column_data = ['unit', 'unit__from_node', unit, input_node, run_name, 'vom_cost']\n",
    "                column_data += [value] * len(df_time)\n",
    "\n",
    "                # Name the column uniquely (e.g., by input_node or unit)\n",
    "                col_name = f\"{input_node}_costs\"\n",
    "                input_costs_columns[col_name] = column_data\n",
    "\n",
    "# Build the output DataFrame from the columns\n",
    "df_input_costs_data = pd.DataFrame.from_dict(input_costs_columns, orient='columns')\n",
    "\n",
    "df_energy_prices = pd.concat([df_energy_prices, df_input_costs_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c03a893-496e-4c07-951d-cd015570f493",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_parameters_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6190d8",
   "metadata": {},
   "source": [
    "#### Units on costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1031bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add units on costs for all units that have either min_down_time, ramp_up/down_limits, or start_up/shut_down limits/costs\n",
    "# Filter units_parameter_df and df_object__node\n",
    "cost_parameters = ['min_down_time', 'min_up_time', 'shut_down_cost', 'start_up_cost', \n",
    "                   'ramp_down_limit', 'ramp_up_limit', 'start_up_limit', 'shut_down_limit']\n",
    "filtered_units_parameter_df = unit_parameters_df[unit_parameters_df['Parameter'].isin(cost_parameters)]\n",
    "filtered_df_object__node = df_object__node[df_object__node['Parameter'].isin(cost_parameters)]\n",
    "\n",
    "combined_object_names = pd.concat([filtered_units_parameter_df['Object_name'], filtered_df_object__node['Object_name']])\n",
    "unique_objects = combined_object_names.unique()\n",
    "\n",
    "# Check if they already have a units_on_cost value other than 0\n",
    "non_zero_units_df = unit_parameters_df[(unit_parameters_df['Parameter'] == 'units_on_cost') &\n",
    "                                       (unit_parameters_df['Value'] != 0.0)]\n",
    "non_zero_units = non_zero_units_df['Object_name'].unique()\n",
    "\n",
    "# Remove units_on_cost = 0 from unit_parameters_df if in unique_objects\n",
    "unit_parameters_df = unit_parameters_df[~(\n",
    "    (unit_parameters_df['Parameter'] == 'units_on_cost') & \n",
    "    (unit_parameters_df['Value'] == 0) & \n",
    "    (unit_parameters_df['Object_name'].isin(unique_objects))\n",
    ")]\n",
    "\n",
    "unique_units = [unit for unit in unique_objects if unit not in non_zero_units]\n",
    "\n",
    "# Create new columns for each unique unit in df_time_series\n",
    "for unit in unique_units:\n",
    "    unit_data = [\"unit\", run_name, \"units_on_cost\"]\n",
    "    \n",
    "    power_data = df_temp_2['power_wholesale_in'].tolist()\n",
    "    \n",
    "    #df_time_series[unit] = unit_data + power_data\n",
    "    # Reset the index of df_time_series to avoid non-unique index error\n",
    "    df_time_series = df_time_series.reset_index(drop=True)\n",
    "    # Create a temporary DataFrame for the new column\n",
    "    new_column = pd.DataFrame({unit: unit_data + power_data})\n",
    "    # Ensure the new column has the same length as df_time_series\n",
    "    new_column = new_column.reindex(df_time_series.index)\n",
    "    # Use pd.concat to add new columns without overwriting existing ones\n",
    "    df_time_series = pd.concat([df_time_series, new_column], axis=1)\n",
    "\n",
    "\n",
    "# Show table head for control\n",
    "df_time_series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df00bdab-f254-437f-b78d-d32b19e2f9db",
   "metadata": {},
   "source": [
    "#### Time Series Storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfefe35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Date index\n",
    "start_date = pd.to_datetime(start_date)\n",
    "before = start_date-timedelta(hours=1)\n",
    "formatted_before = before.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "df_beginning = pd.DataFrame({'DateTime': ['Alternative', 'Parameter', formatted_before]})\n",
    "\n",
    "df_temp4 = pd.DataFrame(columns=['DateTime'])\n",
    "df_temp4 = pd.concat([df_beginning, df_time], ignore_index = True)\n",
    "\n",
    "# Get storage values for fix_node_state\n",
    "storage_values = df_model_storages.iloc[:,[0, 2, 6]]\n",
    "storage_values = storage_values.copy()\n",
    "storage_values['Alternative'] = run_name\n",
    "storage_values = storage_values.iloc[:, [0, 3, 2, 1]]\n",
    "storage_values_transposed = storage_values.T\n",
    "storage_values_transposed.columns = storage_values_transposed.iloc[0]\n",
    "storage_values_transposed = storage_values_transposed[1:]\n",
    "storage_values_transposed.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_storage = pd.concat([df_temp4, storage_values_transposed], axis=1)\n",
    "df_storage.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c562b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add storage if roll_forward and a min_shut_down_period are used\n",
    "product_tr = translate_product(product)\n",
    "last_storage = f\"{product_tr}_st\"\n",
    "indices = list(range(1, int(num_slices)))\n",
    "\n",
    "if \"min_down_time\" in unit_parameters_duration_df['Parameter'].values and roll_forward_use:\n",
    "    new_column = pd.DataFrame({last_storage: [run_name, 'node_state_min']})\n",
    "    # Add to the rest of the storage df\n",
    "    df_storage = pd.concat([df_storage, new_column], axis=1)\n",
    "    \n",
    "    # Down time\n",
    "    min_down_df = unit_parameters_duration_df[unit_parameters_duration_df['Parameter'].isin({'min_down_time'})]\n",
    "    min_down_df['Value'] = min_down_df['Value'].str[:-1].astype(float)\n",
    "    time_down = max(min_down_df['Value'])\n",
    "    \n",
    "    # Demand hourly\n",
    "    demand = df_nodes.loc[df_nodes['Object_name'] == f\"{product_tr}_demand\", 'demand'].values\n",
    "    demand = demand[0]\n",
    "    \n",
    "    storage_needed = demand * time_down\n",
    "    \n",
    "    # Add storage_needed at relevant indices and set the next cell to 0\n",
    "    for i in indices:\n",
    "        index_value = (i * roll_forward_size) + 1 #plus one as it starts in the last hour of the prior period\n",
    "        df_storage.iloc[index_value, -1] = storage_needed\n",
    "        \n",
    "        # Set the cell after storage_needed to 0 if it exists within bounds\n",
    "        next_index = index_value + 1\n",
    "        if next_index < len(df_storage):\n",
    "            df_storage.iloc[next_index, -1] = 0\n",
    "    \n",
    "# Show table head for control\n",
    "df_storage.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dfbf3e-1928-4547-8b3a-dac782ca46a3",
   "metadata": {},
   "source": [
    "## Creating one combined excel and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5523a4cd-ad6f-44d6-993d-3ebaf00680c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prepared input excel for the use in SpineToolbox\n",
    "with pd.ExcelWriter(output_file_path + output_file_name) as writer:\n",
    "    df_definition.to_excel(writer, sheet_name='Definition', index=False)\n",
    "    unit_parameters_rest_df.to_excel(writer, sheet_name='Definition_parameters', index=False)\n",
    "    unit_parameters_duration_df.to_excel(writer, sheet_name='Definition_parameters_duration', index=False)\n",
    "    df_units_inv_parameters.to_excel(writer, sheet_name='Unit_Inv_Parameters', index=False)\n",
    "    df_nodes.to_excel(writer, sheet_name='Nodes', index=False)\n",
    "    df_connections_inv_parameters.to_excel(writer, sheet_name='Connection_Inv_Parameters', index=False)\n",
    "    df_storages_inv_parameters.to_excel(writer, sheet_name='Storage_Inv_Parameters', index=False)\n",
    "    df_object__node_definitions.to_excel(writer, sheet_name='Object__to_from_node_definition', index=False)\n",
    "    df_object__node_values.to_excel(writer, sheet_name='Object__to_from_node', index=False)\n",
    "    df_boolean_relations.to_excel(writer, sheet_name='Boolean_relations', index=False)\n",
    "    df_object_node_node.to_excel(writer, sheet_name='Object__node_node', index=False)\n",
    "    df_variable_eff_def.to_excel(writer, sheet_name='Variable_Eff_Definition', index=False)\n",
    "    df_variable_efficiency.to_excel(writer, sheet_name='Variable_Eff', index=False)\n",
    "    df_operating_points.to_excel(writer, sheet_name='Operating_points', index=False)\n",
    "    df_storage.to_excel(writer, sheet_name='Time_series_storage', index=False)\n",
    "    df_time_series.to_excel(writer, sheet_name='Time_series', index=False)\n",
    "    df_energy_prices.to_excel(writer, sheet_name='Energy_prices', index=False)\n",
    "    df_model_components.to_excel(writer, sheet_name='Model_components', index=False)\n",
    "    df_model_relations.to_excel(writer, sheet_name='Model_relations', index=False)\n",
    "    df_model.to_excel(writer, sheet_name='Model', index=False)\n",
    "    df_temporal_relations.to_excel(writer, sheet_name='Temporal_relations', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
